{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e35972-31dc-4fbd-a080-e3789061e8ac",
   "metadata": {},
   "source": [
    "# Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f61f182-08ce-40ee-b95c-5f3356147cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import torchvision.models\n",
    "import torchvision.datasets\n",
    "import os\n",
    "from collections import Counter\n",
    "# https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37fb1c7-c02b-41ad-8d14-50e9cd3a0ece",
   "metadata": {},
   "source": [
    "## Locate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ba9b11-33d1-42c2-a1f4-0c5d31fa53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data_small' # uses directory to find image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0133c42-d987-4786-a20d-865747106971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['breast_cancer']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18caf7-3344-4d2a-8557-8bcca58f1cf4",
   "metadata": {},
   "source": [
    "## Transform and Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc60bb25-3cbd-4c8b-8070-c3ba8417d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([ # allows me to use Resize and ToTensor together\n",
    "    torchvision.transforms.Resize((50, 50)), # Resizes the images to match Kaggle size description\n",
    "    torchvision.transforms.ToTensor()]) #converts image to multi-dim matrix\n",
    "all_images = []\n",
    "for image in os.listdir(data_dir):\n",
    "      all_images.append(torchvision.datasets.ImageFolder(os.path.join(data_dir, image), transform=transform)) \n",
    "        # loads all images\n",
    "datasets = torch.utils.data.ConcatDataset(all_images) # creates a concat dataset which holds all images, file names, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead815b7-7ed9-46a4-a5b2-9be7c4c28a2f",
   "metadata": {},
   "source": [
    "## Determines number of negative and positive cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eebe3ba-e04e-4dea-8434-02421b5eeb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Images in files '0' and '1':\n",
      "    number of images in file '0' (Negative for Breast Cancer): 4268 \n",
      "    number of images in file '1' (Positive for Breasr Cancer): 1872\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for dataset in datasets.datasets: # dataset opens a patient id which contains files '0' and '1'\n",
    "    if i==0: # opens file '0' which contains images with negative results\n",
    "        result = Counter(dataset.targets) # counts the number of images in '0' folder\n",
    "        i += 1 # opnes file '1' which contains images with positive results\n",
    "    else:\n",
    "        result += Counter(dataset.targets) # counts the number of images in '1' folder\n",
    "\n",
    "result = dict(result) # creates a dictionary of the number of positive and negative results\n",
    "print(\"\"\"Total Number of Images in files '0' and '1':\n",
    "    number of images in file '0' (Negative for Breast Cancer): {} \n",
    "    number of images in file '1' (Positive for Breasr Cancer): {}\"\"\".format(result[0], result[1]))\n",
    "# images in file '0' are negative\n",
    "# images in file '1' are positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fdf045d-d25e-4dc3-b24d-35c42c93f576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1999e5746d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5400e5c-eeb2-403f-9441-942a312da7da",
   "metadata": {},
   "source": [
    "## Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9bb8723-2d33-429d-9bde-beaf2cc80132",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result = result[0] + result[1] # adds total number of images\n",
    "train_size = int(0.8*(total_result)) # 80% training set\n",
    "test_size = total_result-train_size # 20% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3571e940-18ad-4b6e-8662-eaaf9788b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(datasets, [train_size, test_size])\n",
    "# randomly splits the dataset with 80% training and 20% test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2571c0c-9009-4f31-bee0-66b7f8d092ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "# Dataloaders allow for more convenient access to the images\n",
    "# shuffle is done on training dataset for better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d69ba8-b79d-4505-92aa-617026bffdce",
   "metadata": {},
   "source": [
    "## Use GPU if it has CUDA Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b535c66-72bc-44bc-9cff-985afd9a9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "    return torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9af471-8625-4884-9cee-bdf49ffebb39",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fa4f892-b399-40e6-8524-473a5e693a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, train_loader, test_loader, criterion, optimizer, n_epochs):\n",
    "    device = set_device()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch number %d \" % (epoch + 1))\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        total = 0\n",
    "\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            total += labels.size(0)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            loss.backward()\n",
    "    \n",
    "            optimizer.step()\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "            running_correct += (labels==predicted).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss/len(trainloader)\n",
    "        epoch_accuracy = 100 * running_correct / total\n",
    "\n",
    "        print(\" - Training Dataset. Got %d out of %d images correctly (%.3f%%). Epoch loss: %.3f\"\n",
    "             % (running_correct, total, epoch_accuracy, epoch_loss))\n",
    "\n",
    "        evaluate_model_on_test_set(model, testloader)\n",
    "\n",
    "    print(\"Finished\")\n",
    "    return model                          \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79d2146d-d04f-4308-87d2-996c1e6dd0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test_set(model, testloader):\n",
    "    model.eval()\n",
    "    predicted_correctly_on_epoch = 0\n",
    "    total = 0\n",
    "    device = set_device()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            total += labels.size(0)\n",
    "    \n",
    "            outputs = model(images)\n",
    "    \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "            predicted_correctly_on_epoch += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_accuracy = 100.0 * predicted_correctly_on_epoch / total\n",
    "    print(\" - Testing Dataset. Got %d out of %d images correctly (%.3f%%)\"\n",
    "         % (predicted_correctly_on_epoch, total, epoch_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6870be9-3470-42e2-b674-2ebfbd369674",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_model = torchvision.models.resnet18(pretrained=True) # uses pretrained model and weights\n",
    "num_features = resnet18_model.fc.in_features # size of each input sample\n",
    "num_of_classifiers = 2 # classifies 0 and 1 \n",
    "resnet18_model.fc = torch.nn.Linear(num_features, num_of_classifiers) \n",
    "# applies a linear transformation using the num_features (input) and num_of_classifiers (output) to generate the output\n",
    "device = set_device() # either gpu or cpu will be used \n",
    "resnet_18_model = resnet18_model.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # determines our error between expected output and actual output\n",
    "optimizer = torch.optim.SGD(resnet18_model.parameters(), lr = 0.01, momentum = 0.9, weight_decay = 0.01) \n",
    "# Stochastic gradient descent \n",
    "# momentum helps point the gradient vectors to the right direction\n",
    "# weight decay helps to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f180777-66ef-4347-8df7-c69586b59d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1 \n",
      " - Training Dataset. Got 4037 out of 4912 images correctly (82.186%). Epoch loss: 0.410\n",
      " - Testing Dataset. Got 1051 out of 1228 images correctly (85.586%)\n",
      "Epoch number 2 \n",
      " - Training Dataset. Got 4452 out of 4912 images correctly (90.635%). Epoch loss: 0.222\n",
      " - Testing Dataset. Got 1054 out of 1228 images correctly (85.831%)\n",
      "Epoch number 3 \n",
      " - Training Dataset. Got 4683 out of 4912 images correctly (95.338%). Epoch loss: 0.119\n",
      " - Testing Dataset. Got 1068 out of 1228 images correctly (86.971%)\n",
      "Epoch number 4 \n",
      " - Training Dataset. Got 4754 out of 4912 images correctly (96.783%). Epoch loss: 0.080\n",
      " - Testing Dataset. Got 1044 out of 1228 images correctly (85.016%)\n",
      "Epoch number 5 \n",
      " - Training Dataset. Got 4836 out of 4912 images correctly (98.453%). Epoch loss: 0.047\n",
      " - Testing Dataset. Got 1066 out of 1228 images correctly (86.808%)\n",
      "Epoch number 6 \n",
      " - Training Dataset. Got 4792 out of 4912 images correctly (97.557%). Epoch loss: 0.064\n",
      " - Testing Dataset. Got 1062 out of 1228 images correctly (86.482%)\n",
      "Epoch number 7 \n",
      " - Training Dataset. Got 4842 out of 4912 images correctly (98.575%). Epoch loss: 0.041\n",
      " - Testing Dataset. Got 1074 out of 1228 images correctly (87.459%)\n",
      "Epoch number 8 \n",
      " - Training Dataset. Got 4833 out of 4912 images correctly (98.392%). Epoch loss: 0.046\n",
      " - Testing Dataset. Got 1048 out of 1228 images correctly (85.342%)\n",
      "Epoch number 9 \n",
      " - Training Dataset. Got 4854 out of 4912 images correctly (98.819%). Epoch loss: 0.036\n",
      " - Testing Dataset. Got 1061 out of 1228 images correctly (86.401%)\n",
      "Epoch number 10 \n",
      " - Training Dataset. Got 4873 out of 4912 images correctly (99.206%). Epoch loss: 0.026\n",
      " - Testing Dataset. Got 1044 out of 1228 images correctly (85.016%)\n",
      "Epoch number 11 \n",
      " - Training Dataset. Got 4835 out of 4912 images correctly (98.432%). Epoch loss: 0.044\n",
      " - Testing Dataset. Got 1055 out of 1228 images correctly (85.912%)\n",
      "Epoch number 12 \n",
      " - Training Dataset. Got 4814 out of 4912 images correctly (98.005%). Epoch loss: 0.054\n",
      " - Testing Dataset. Got 1077 out of 1228 images correctly (87.704%)\n",
      "Epoch number 13 \n",
      " - Training Dataset. Got 4863 out of 4912 images correctly (99.002%). Epoch loss: 0.029\n",
      " - Testing Dataset. Got 1075 out of 1228 images correctly (87.541%)\n",
      "Epoch number 14 \n"
     ]
    }
   ],
   "source": [
    "train_nn(resnet18_model, trainloader, testloader, loss_fn, optimizer, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b278c8bb-9186-4fb2-ad04-63c083ddd33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 89.938% best testing score using batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7594216-b5c5-4c81-9991-8f0098034ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90.065% 128 lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e7b23-e6b4-4af9-8214-27f2d3c2459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 86.564% 128 lr = 0.01 w/o momentum or decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da16a6-a22f-4d1d-9f5b-37d6e9c2c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 88.925% 128 lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631eb54-7516-49a6-9a25-5ccaeadc1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 87.866% 128 lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a0e2d-96e9-4bf6-b48b-1581559842f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 88.76% 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957cafd5-a2d9-408c-8cad-cbe1009f8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 87.948% 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec7bff71-a1ca-401f-8c72-534b2f3696ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 87.134% 64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
